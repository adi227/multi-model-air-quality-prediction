{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgTZcWSgVIX8"
      },
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "!pip install -q openpyxl scikit-learn matplotlib seaborn\n",
        "!pip install scikeras\n",
        "\n",
        "\n",
        "# Import core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import deep learning tools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization\n",
        "from scikeras.wrappers import KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Upload your dataset (Excel file) in Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "EthkB9P8bm5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Step 2.1 Once uploaded, read the Excel file (replace the filename if different)\n",
        "filename = list(uploaded.keys())[0]\n",
        "data = pd.read_excel(io.BytesIO(uploaded[filename]),\n",
        "                     sheet_name='Agrofood_co2_emission_imputedKN')\n"
      ],
      "metadata": {
        "id": "pDDymSo4cbal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Preprocessing â€“ Drop Non-Numeric / Low-Relevance Columns\n",
        "data_cleaned = data.drop(columns=[\"Region\", \"Area\", \"Year\"])\n",
        "\n",
        "# Remove features with low correlation to target\n",
        "cor_matrix = data_cleaned.corr()\n",
        "low_corr = cor_matrix[\"total_emission\"].abs()[cor_matrix[\"total_emission\"].abs() < 0.2].index.tolist()\n",
        "\n",
        "#  Safely remove the target column if present\n",
        "if \"total_emission\" in low_corr:\n",
        "    low_corr.remove(\"total_emission\")\n",
        "\n",
        "print(f\"Features dropped due to low correlation: {low_corr}\")\n",
        "data_cleaned = data_cleaned.drop(columns=low_corr)\n"
      ],
      "metadata": {
        "id": "gtqnDx95cid_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 3: Define Features and Target + Train/Test Split\n",
        "# Define feature matrix X and target y\n",
        "X = data_cleaned.drop(columns=[\"total_emission\"])\n",
        "y = data_cleaned[\"total_emission\"]\n",
        "\n",
        "# Split the dataset (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "8o3TWTYKdsc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 4: Feature Scaling\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "rDxLYnPcd0tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 5: Build and Train MLP Model\n",
        "# Define the MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Linear output for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Add early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "4oRU-hDed6z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Plot Training & Validation Loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('MLP Training & Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3PMHCoQZe7pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Evaluate Model Performance on Test Set\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test_scaled).flatten()\n",
        "\n",
        "# Compute metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nðŸ” Performance Metrics (MLP - Test Set):\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RÂ²: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "dMxNULLwfIis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Visualize Actual vs Predicted\n",
        "results = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(x=\"Actual\", y=\"Predicted\", data=results, alpha=0.6)\n",
        "plt.plot([results.min().min(), results.max().max()], [results.min().min(), results.max().max()],\n",
        "         linestyle=\"--\", color=\"red\")\n",
        "plt.title(\"Actual vs Predicted COâ‚‚ Emissions (MLP)\")\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q42HSF27fT2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9  Visualize Feature Importance with Permutation Importance\n",
        "try:\n",
        "    import eli5\n",
        "except ImportError:\n",
        "    !pip install eli5\n",
        "    import eli5\n",
        "\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "\n",
        "perm = PermutationImportance(estimator, random_state=42).fit(X_test_scaled, y_test)\n",
        "eli5.show_weights(perm, feature_names=X.columns.tolist())\n"
      ],
      "metadata": {
        "id": "95TBBsozl4hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Visualize Residuals\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(residuals, kde=True, color='orange')\n",
        "plt.axvline(0, linestyle='--', color='black')\n",
        "plt.title(\"Residuals Distribution\")\n",
        "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xtgZdNNjsao2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}